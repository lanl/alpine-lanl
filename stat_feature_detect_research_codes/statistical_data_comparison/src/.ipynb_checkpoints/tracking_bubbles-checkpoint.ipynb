{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import vtk\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from vtk.util.numpy_support import *\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from vtk.util import numpy_support\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import linalg as LA\n",
    "from skimage.measure import EllipseModel\n",
    "from matplotlib.patches import Ellipse\n",
    "import pickle\n",
    "from sklearn import manifold\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multiblock(filename):\n",
    "    reader = vtk.vtkXMLMultiBlockDataReader()\n",
    "    reader.SetFileName(filename)\n",
    "    reader.Update()\n",
    "    return reader.GetOutput()\n",
    "    \n",
    "def write_multiblock(filename,data):\n",
    "    writer = vtk.vtkXMLMultiBlockDataWriter()\n",
    "    writer.SetInputData(data)\n",
    "    writer.SetFileName(filename)\n",
    "    writer.Update()\n",
    "\n",
    "def ensureUtf(s):\n",
    "    try:\n",
    "        if type(s) == unicode:\n",
    "            return s.encode('utf8', 'ignore')\n",
    "    except: \n",
    "        return str(s)\n",
    "    \n",
    "def read_vti(filename):\n",
    "    reader = vtk.vtkXMLImageDataReader()\n",
    "    reader.SetFileName(filename)\n",
    "    reader.Update()\n",
    "    return reader.GetOutput()\n",
    "\n",
    "def write_vti(filename,data):\n",
    "    writer = vtk.vtkXMLImageDataWriter()\n",
    "    writer.SetInputData(data)\n",
    "    writer.SetFileName(filename)\n",
    "    writer.Update()\n",
    "    \n",
    "def compute_3d_to_1d_map(x,y,z,dimx,dimy,dimz):\n",
    "    return x + dimx*(y+dimy*z)    \n",
    "    \n",
    "def extract_target_feature(feature_volume,confidence_th,feature_id):\n",
    "    \n",
    "    feature_volume.GetPointData().SetActiveScalars('feature_similarity')\n",
    "    thresholding = vtk.vtkThreshold()\n",
    "    thresholding.ThresholdByUpper(confidence_th)\n",
    "    thresholding.SetInputData(feature_volume)\n",
    "    seg = vtk.vtkConnectivityFilter()\n",
    "    seg.SetInputConnection(thresholding.GetOutputPort())\n",
    "    seg.SetExtractionModeToAllRegions()\n",
    "    seg.ColorRegionsOn()\n",
    "    thresholding2 = vtk.vtkThreshold()\n",
    "    thresholding2.SetInputConnection(seg.GetOutputPort())\n",
    "    thresholding2.ThresholdBetween(feature_id,feature_id)\n",
    "    thresholding2.Update()\n",
    "    return thresholding2.GetOutput()\n",
    "\n",
    "def compute_1d_indices(single_feature,dims,gbounds):\n",
    "    \n",
    "    oneD_indices = [] \n",
    "    \n",
    "    for i in range(single_feature.GetNumberOfPoints()):\n",
    "        pts = single_feature.GetPoint(i)\n",
    "        xval = int(((pts[0] - gbounds[0])/(gbounds[1]-gbounds[0]))*dims[0])\n",
    "        yval = int(((pts[1] - gbounds[2])/(gbounds[3]-gbounds[2]))*dims[1])\n",
    "        zval = int(((pts[2] - gbounds[4])/(gbounds[5]-gbounds[4]))*dims[2])\n",
    "        \n",
    "        if xval > dims[0]-1:\n",
    "            xval = dims[0]-1\n",
    "        \n",
    "        if yval > dims[1]-1:\n",
    "            yval = dims[1]-1\n",
    "            \n",
    "        if zval > dims[2]-1:\n",
    "            zval = dims[2]-1    \n",
    "        \n",
    "        index = compute_3d_to_1d_map(xval,yval,zval,dims[0],dims[1],dims[2])\n",
    "        oneD_indices.append(index)\n",
    "        \n",
    "        if index > dims[0]*dims[1]*dims[2] or index < 0:\n",
    "            print (xval,yval,zval)\n",
    "    \n",
    "    if len(oneD_indices) != single_feature.GetNumberOfPoints():\n",
    "        print ('mismatch in mumber of points!!')\n",
    "        \n",
    "    return oneD_indices\n",
    "\n",
    "\n",
    "def find_match(data,target_feature,confidence_th,dims,size_threshold,gbounds,pickle_data):\n",
    "    \n",
    "    num_features = data.GetNumberOfBlocks()\n",
    "    \n",
    "    matched = []\n",
    "    for i in range(num_features):\n",
    "        block = data.GetBlock(i)\n",
    "        \n",
    "        vtk_pts = block.GetPoints()\n",
    "        num_feature_pts = vtk_pts.GetNumberOfPoints()\n",
    "        \n",
    "        oneD_indices = []\n",
    "        for j in range(num_feature_pts):\n",
    "            pts = vtk_pts.GetPoint(j)\n",
    "            xval = int(((pts[0] - gbounds[0])/(gbounds[1]-gbounds[0]))*dims[0])\n",
    "            yval = int(((pts[1] - gbounds[2])/(gbounds[3]-gbounds[2]))*dims[1])\n",
    "            zval = int(((pts[2] - gbounds[4])/(gbounds[5]-gbounds[4]))*dims[2])\n",
    "\n",
    "            if xval > dims[0]-1:\n",
    "                xval = dims[0]-1\n",
    "\n",
    "            if yval > dims[1]-1:\n",
    "                yval = dims[1]-1\n",
    "\n",
    "            if zval > dims[2]-1:\n",
    "                zval = dims[2]-1    \n",
    "\n",
    "            index = compute_3d_to_1d_map(xval,yval,zval,dims[0],dims[1],dims[2])\n",
    "            oneD_indices.append(index)\n",
    "\n",
    "            if index > dims[0]*dims[1]*dims[2] or index < 0:\n",
    "                print (xval,yval,zval)\n",
    "        \n",
    "        intersection_cardinality = len(set.intersection(*[set(target_feature), set(oneD_indices)]))\n",
    "        if intersection_cardinality > 0:\n",
    "            matched.append([pickle_data[i][0],pickle_data[i][1],intersection_cardinality,oneD_indices])\n",
    "            break\n",
    "            \n",
    "    return matched\n",
    "\n",
    "\n",
    "def check_merge(feature_id_temp,target_feature_1d_pts,ctstep,inpfname,pickle_fname,confidence_th,size_threshold,dims,gbounds):\n",
    "    \n",
    "    ## load features to be tested\n",
    "    current_data = read_multiblock(inpfname)\n",
    "    #load pickle file\n",
    "    ffname = ensureUtf(pickle_fname)\n",
    "    fname = open(ffname, 'rb')\n",
    "    data = pickle.load(fname, encoding='latin1')\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    matched = find_match(current_data,target_feature_1d_pts,confidence_th,dims,size_threshold,gbounds,data)\n",
    "    \n",
    "    idx = int(matched[0][1])\n",
    "    \n",
    "    if len(matched) > 0 and int(feature_id_temp) != idx:\n",
    "        print ('Merge found: tstep: ' + str(matched[0][0]) + ' ' + str(matched[0][0]-75) +  ' fid: ' +  str(matched[0][1]))\n",
    "#     else:\n",
    "#         print ('oppssss: ' + str(matched[0][1]) + ' ' + str(feature_id_temp))\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial feature file to load: ../out/mfix_local_grid/slic_compare_110.vti\n"
     ]
    }
   ],
   "source": [
    "## Parameters\n",
    "confidence_th = 0.92\n",
    "size_threshold = 100\n",
    "dims  = [128,16,128]\n",
    "initT = 75\n",
    "endT = 408\n",
    "\n",
    "# feature_tstep = 193\n",
    "# feature_id = 3\n",
    "# feature_tstep = 120 #(in Paraview should subtract 75)\n",
    "# feature_id = 13\n",
    "feature_tstep = 75 #(in Paraview should subtract 75)\n",
    "feature_id = 1\n",
    "\n",
    "feature_path = '../out/mfix_local_grid/'\n",
    "feature_fname = feature_path + 'slic_compare_' + str(feature_tstep) + '.vti'\n",
    "print ('initial feature file to load: ' + feature_fname)\n",
    "\n",
    "#data_path = '../out/mfix_local_grid/segmented_features/'\n",
    "data_path = '../out/segmented_features/'\n",
    "full_cdb_path = '../bubble_all.cdb/'\n",
    "out_tracked_feature_cdb_path = '../out/feature_cdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge found: tstep: 126.0 51.0 fid: 0.0\n",
      "Merge found: tstep: 143.0 68.0 fid: 0.0\n",
      "No match found for foreward tracking\n",
      "testing for: 109 and 108 3.0\n",
      "testing for: 108 and 107 2.0\n",
      "Merge found: tstep: 108.0 33.0 fid: 1.0\n",
      "testing for: 107 and 106 1.0\n",
      "testing for: 106 and 105 1.0\n",
      "testing for: 105 and 104 1.0\n",
      "testing for: 104 and 103 1.0\n",
      "testing for: 103 and 102 1.0\n",
      "testing for: 102 and 101 1.0\n",
      "testing for: 101 and 100 1.0\n",
      "testing for: 100 and 99 1.0\n",
      "testing for: 99 and 98 1.0\n",
      "testing for: 98 and 97 4.0\n",
      "testing for: 97 and 96 4.0\n",
      "testing for: 96 and 95 7.0\n",
      "testing for: 95 and 94 6.0\n",
      "testing for: 94 and 93 4.0\n",
      "testing for: 93 and 92 4.0\n",
      "testing for: 92 and 91 4.0\n",
      "No match found for backward tracking\n"
     ]
    }
   ],
   "source": [
    "feature_volume = read_vti(feature_fname)\n",
    "gbounds = feature_volume.GetBounds()\n",
    "target_feature = extract_target_feature(feature_volume,confidence_th,feature_id)\n",
    "target_feature_1d_pts_og = compute_1d_indices(target_feature,dims,gbounds)\n",
    "\n",
    "\n",
    "target_feature_1d_pts = target_feature_1d_pts_og\n",
    "feature_id_temp = feature_id\n",
    "\n",
    "## Forward tracking\n",
    "\n",
    "forward_tracking = []\n",
    "for ii in range(feature_tstep+1,endT):\n",
    "    ## load features to be tested\n",
    "    inpfname = data_path + 'segmented_' + str(ii) + '.vtm'\n",
    "    current_data = read_multiblock(inpfname)\n",
    "    \n",
    "    #load pickle file\n",
    "    pickle_fname = data_path + 'feature_values_' + str(ii) + '.pickle'\n",
    "    ffname = ensureUtf(pickle_fname)\n",
    "    fname = open(ffname, 'rb')\n",
    "    data = pickle.load(fname, encoding='latin1')\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    matched = find_match(current_data,target_feature_1d_pts,confidence_th,dims,size_threshold,gbounds,data)\n",
    "    \n",
    "    if len(matched) > 0:\n",
    "        #print ('Matched feature: tstep: ' + str(matched[0][0]) + ' ' + str(matched[0][0]-75) +  ' fid: ' +  str(matched[0][1]))\n",
    "        ## update the target feature\n",
    "        target_feature_1d_pts = matched[0][3]\n",
    "        \n",
    "        ### Check merge/split\n",
    "        if ii > feature_tstep+1:\n",
    "            ctstep = ii-1\n",
    "            inpfname1 = data_path + 'segmented_' + str(ctstep) + '.vtm'\n",
    "            pickle_fname1 = data_path + 'feature_values_' + str(ctstep) + '.pickle'\n",
    "            retval = check_merge(feature_id_temp,target_feature_1d_pts,ctstep,inpfname1,pickle_fname1,confidence_th,size_threshold,dims,gbounds)\n",
    "#         else:\n",
    "#             ## process the initial timestep here\n",
    "#             print ('here')\n",
    "        \n",
    "        feature_id_temp = matched[0][1]\n",
    "        forward_tracking.append([matched[0][0],matched[0][1],matched[0][2]])\n",
    "    else:\n",
    "        print ('No match found for foreward tracking')\n",
    "        break   \n",
    "\n",
    "        \n",
    "target_feature_1d_pts = target_feature_1d_pts_og\n",
    "feature_id_temp = feature_id\n",
    "\n",
    "## Backward tracking\n",
    "backward_tracking = []\n",
    "for jj in range(feature_tstep-1,initT,-1):\n",
    "    \n",
    "    ## load features to be tested\n",
    "    inpfname = data_path + 'segmented_' + str(jj) + '.vtm'\n",
    "    current_data = read_multiblock(inpfname)\n",
    "    \n",
    "    #load pickle file\n",
    "    pickle_fname = data_path + 'feature_values_' + str(jj) + '.pickle'\n",
    "    ffname = ensureUtf(pickle_fname)\n",
    "    fname = open(ffname, 'rb')\n",
    "    data = pickle.load(fname, encoding='latin1')\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    matched = find_match(current_data,target_feature_1d_pts,confidence_th,dims,size_threshold,gbounds,data)\n",
    "    \n",
    "    if len(matched) > 0:\n",
    "        #print ('Matched feature: tstep: ' + str(matched[0][0]) + ' ' + str(matched[0][0]-75) +  ' fid: ' +  str(matched[0][1]))\n",
    "        ## update the target feature\n",
    "        \n",
    "        target_feature_1d_pts = matched[0][3]\n",
    "        \n",
    "        ### Check merge/split\n",
    "        if jj < feature_tstep-1:\n",
    "            \n",
    "            ctstep = jj+1\n",
    "            print ('testing for: ' + str(ctstep) + ' and ' + str(jj) + ' ' + str(feature_id_temp))\n",
    "                   \n",
    "            inpfname1 = data_path + 'segmented_' + str(ctstep) + '.vtm'\n",
    "            pickle_fname1 = data_path + 'feature_values_' + str(ctstep) + '.pickle'\n",
    "            \n",
    "            retval = check_merge(feature_id_temp,target_feature_1d_pts,ctstep,inpfname1,pickle_fname1,confidence_th,size_threshold,dims,gbounds)\n",
    "#         else:\n",
    "#             ## process the initial timestep here\n",
    "#             print ('here')\n",
    "        \n",
    "        feature_id_temp = matched[0][1]\n",
    "        backward_tracking.append([matched[0][0],matched[0][1],matched[0][2]])\n",
    "    else:\n",
    "        print ('No match found for backward tracking')\n",
    "        break\n",
    "\n",
    "## first combine all tracking results\n",
    "all_tracked_results = backward_tracking + forward_tracking\n",
    "all_tracked_results.append([feature_tstep,feature_id,len(target_feature_1d_pts_og)])\n",
    "\n",
    "## sort the results by time step number\n",
    "all_tracked_results.sort(key = lambda all_tracked_results: all_tracked_results[0]) \n",
    "#print (all_tracked_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1  2         3         4     5           6                        7\n",
      "0     91  6  0.824661  0.908838   125  0.00250248   images/bubble_6_91.png\n",
      "1     92  4  0.767992  0.885315   174  0.00309833   images/bubble_4_92.png\n",
      "2     93  4   0.60971   0.92097   220  0.00375276   images/bubble_4_93.png\n",
      "3     94  4  0.754823  0.932327   248  0.00433877   images/bubble_4_94.png\n",
      "4     95  6  0.596419  0.954819   297  0.00525143   images/bubble_6_95.png\n",
      "..   ... ..       ...       ...   ...         ...                      ...\n",
      "100  191  4  0.997112  0.707344  3837    0.064026  images/bubble_4_191.png\n",
      "101  192  3  0.972306  0.716295  4008   0.0652065  images/bubble_3_192.png\n",
      "102  193  3   0.91949  0.723754  4123   0.0664912  images/bubble_3_193.png\n",
      "103  194  3  0.955579  0.745644  4151    0.067945  images/bubble_3_194.png\n",
      "104  195  5  0.941376  0.720621  4256   0.0697911  images/bubble_5_195.png\n",
      "\n",
      "[105 rows x 7 columns]\n",
      "done tracking\n"
     ]
    }
   ],
   "source": [
    "## load the complete database and filter the target specific cinemadatabase\n",
    "\n",
    "##load csv file in dataframe\n",
    "fname = full_cdb_path+'data.csv'\n",
    "df = pd.read_csv(fname)\n",
    "\n",
    "filtered_data=[]\n",
    "for i in range(len(all_tracked_results)):\n",
    "    data = df[(df['time_step']==all_tracked_results[i][0]) & (df['feature_id']==all_tracked_results[i][1])]\n",
    "    data = data.to_numpy()\n",
    "    data_list=list(data)    \n",
    "    filtered_data.append(data_list)\n",
    "\n",
    "## strip off one dimension, no changes to the actual data\n",
    "final_filtered_data_tmp=[]\n",
    "for i in range(np.shape(filtered_data)[0]):\n",
    "    final_filtered_data_tmp.append(filtered_data[i][0])\n",
    "\n",
    "# filter unexpected feature (from the top most layer most of the time)\n",
    "final_filtered_data = []\n",
    "for i in range(np.shape(final_filtered_data_tmp)[0]-1):\n",
    "    if final_filtered_data_tmp[i][6] < final_filtered_data_tmp[i+1][6]:\n",
    "        final_filtered_data.append(final_filtered_data_tmp[i])\n",
    "    else:\n",
    "        final_filtered_data.append(final_filtered_data_tmp[i])\n",
    "        #break\n",
    "    \n",
    "#print (np.shape(final_filtered_data_tmp))    \n",
    "final_filtered_data = np.asarray(final_filtered_data)\n",
    "\n",
    "## clean the old result\n",
    "os.system('rm -rf ' + out_tracked_feature_cdb_path + '*')\n",
    "\n",
    "# mode for the folder \n",
    "mode = 0o777\n",
    "out_feature_cdb_fname = out_tracked_feature_cdb_path + 'bubble.cdb'\n",
    "os.mkdir(out_feature_cdb_fname, mode)\n",
    "\n",
    "images_path = out_feature_cdb_fname + '/images'\n",
    "os.mkdir(images_path, mode)\n",
    "\n",
    "df = pd.DataFrame(final_filtered_data, index=range(final_filtered_data.shape[0]),\n",
    "                          columns=range(final_filtered_data.shape[1]))\n",
    "\n",
    "# ## drop the columns that has df column ids from previous data frame and serial ids\n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "print (df)\n",
    "\n",
    "## Name the columns\n",
    "df.rename(columns={1:'time_step'}, inplace=True)\n",
    "df.rename(columns={2:'feature_id'}, inplace=True)\n",
    "df.rename(columns={3:'aspect_ratio'}, inplace=True)\n",
    "df.rename(columns={4:'eigen_ratio'}, inplace=True)\n",
    "df.rename(columns={5:'volume'}, inplace=True)\n",
    "df.rename(columns={6:'x_center'}, inplace=True)\n",
    "df.rename(columns={7:'FILE'}, inplace=True)\n",
    "\n",
    "## store into csv file\n",
    "out_csv_path = '/Users/sdutta/Codes/cinema_explorer/data/bubble.cdb' + '/data.csv'\n",
    "#out_csv_path = out_feature_cdb_fname + '/data.csv'\n",
    "df.to_csv(out_csv_path,index=False)\n",
    "\n",
    "print ('done tracking')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
